stages:
  - terraform

terraform_job:
  stage: terraform

  before_script:
  - echo "Installing Terraform..."
  - apt-get update && apt-get install -y wget unzip
  - wget https://releases.hashicorp.com/terraform/${TF_VERSION}/terraform_${TF_VERSION}_linux_amd64.zip
  - unzip terraform_${TF_VERSION}_linux_amd64.zip -d /usr/local/bin/
  - chmod +x /usr/local/bin/terraform
  - terraform --version  # Check Terraform version
  
  script:
    - echo "Starting Terraform Job"
    - echo "Initializing Terraform..."
    - cd $TF_WORKING_DIR  # Change to the Terraform directory
    - terraform init
    - echo "Listing available Terraform workspaces..."
    - terraform workspace list  # List available workspaces
    - echo "Selecting or creating 'test' workspace..."
    - terraform workspace new test || terraform workspace select test  # Create "test" workspace if it doesn't exist, else select it
    - echo "Planning Terraform changes..."
    - terraform plan
    - echo "Applying Terraform changes..."
    - terraform apply -auto-approve
    - echo "Extracting outputs from Terraform..."
    - echo "PUBLIC_IP=$(terraform output -raw bastion_public_ip)" > variables.env  # Use > to overwrite
    - echo "PRIVATE_IP=$(terraform output -raw private_ec2_private_ip)" >> variables.env
    - echo "ECR_REPOSITORY_RDS=$(terraform output -raw ecr_repository_rds)" >> variables.env
    - echo "ECR_REPOSITORY_REDIS=$(terraform output -raw ecr_repository_redis)" >> variables.env
    - cp petp_keypair-test private_key.pem  # Copy the generated key
    - chmod 600 private_key.pem  # Make the key available for SSH
    
    # Verification steps to ensure the files exist
    - echo "Checking current directory and files..."
    - pwd  # Print the current directory
    - ls -la  # Verify the contents of the current directory
    - echo "Contents of variables.env:"
    - cat variables.env  # Output contents of variables.env for debugging
    
    # Ensure that the files are moved to the working directory if necessary
    - echo "Moving artifacts to working directory if needed..."
    - mv variables.env private_key.pem $CI_PROJECT_DIR/ || true  # Move files if they exist
    
    - echo "Listing files to verify variables.env and private_key.pem exist..."
    - ls -la $CI_PROJECT_DIR/  # Verify if variables.env and private_key.pem are in the working directory

  artifacts:
    paths:
      - variables.env  # Include the variables.env file
      - private_key.pem  # Transmit the key as an artifact
      - $TF_WORKING_DIR/.terraform  # Persist the .terraform directory
      - $TF_WORKING_DIR/terraform.tfstate.d/test/terraform.tfstate  # Persist the state file

    expire_in: 1 day  # Keep artifacts for 1 day
  only:
    - master  # Run this job only on the master branch


# New job to build and push Docker images
# Docker build and push job
docker_build_and_push:
  stage: terraform
  needs: ["terraform_job"]
  image: docker:latest
  services:
    - docker:dind

  before_script:
    - apk add --no-cache python3 py3-pip
    - python3 -m venv /venv  # Create a virtual environment
    - . /venv/bin/activate  # Activate the virtual environment
    - pip install awscli
    - export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
    - export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    - export REGION="eu-north-1"
    - source variables.env  # Load variables from artifacts

    # Install the Amazon ECR Credential Helper
    - apk add --no-cache docker-credential-ecr-login

  # Login to the first repository for backend_rds
    - aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ECR_REPOSITORY_RDS

  script:
    - echo "Checking current directory..."
    - pwd
    - echo "Listing files in the current directory..."
    - ls -la
    
  # For backend_rds
    - echo "Building and pushing Docker image for backend_rds..."
    - cd backend_rds
    - pwd
    - docker build -t backend_rds .  
    - docker tag backend_rds:latest $ECR_REPOSITORY_RDS:latest
    - docker push $ECR_REPOSITORY_RDS:latest
    - cd ..

  # Login to the second repository for backend_redis
    - aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ECR_REPOSITORY_REDIS

  # For backend_redis
    - echo "Building and pushing Docker image for backend_redis..."
    - cd backend_redis
    - pwd
    - docker build -t backend_redis .
    - docker tag backend_redis:latest $ECR_REPOSITORY_REDIS:latest
    - docker push $ECR_REPOSITORY_REDIS:latest
  
  needs:
    - job: terraform_job
      artifacts: true  # Ensure it gets the artifacts from terraform_job
  artifacts:
    paths:
      - variables.env  # Include the variables.env file
      - private_key.pem  # Transmit the key as an artifact
      - $TF_WORKING_DIR/.terraform  # Persist the .terraform directory
      - $TF_WORKING_DIR/terraform.tfstate.d/test/terraform.tfstate  # Persist the state file
  only:
    - master  # Run this job only on the master branch




deploy_application:
  stage: terraform
  needs:
    - terraform_job
    - docker_build_and_push
  image: alpine:latest
  before_script:
    - echo "Installing required packages..."
    - apk add --no-cache openssh-client docker-cli docker-credential-ecr-login
    - echo "Loading environment variables..."
    - source variables.env
    - chmod 600 private_key.pem  # Ensure private key permissions are correct
    - echo "Starting SSH agent..."
    - eval $(ssh-agent)  # Start ssh-agent for key management
    - echo "Adding private key to SSH agent..."
    - ssh-add private_key.pem  # Add private key for SSH access
  script:
    - echo "Connecting to Bastion Host with SSH agent forwarding..."
    - ssh -A -o StrictHostKeyChecking=no ec2-user@$PUBLIC_IP "
      echo 'Checking Bastion Hostname:';
      hostname;  
      echo 'Connecting from Bastion to Private EC2 Instance...'; 
      ssh -o StrictHostKeyChecking=no ec2-user@$PRIVATE_IP '
        echo \"Connected to Private EC2 Instance\";
        echo \"Checking Private EC2 Instance Hostname:\";
        hostname;  

        echo \"Configuring AWS CLI...\";
        aws configure set aws_access_key_id ${AWS_ACCESS_KEY_ID};
        aws configure set aws_secret_access_key ${AWS_SECRET_ACCESS_KEY};
        aws configure set region eu-north-1;

        echo \"Logging in to ECR...\";
        aws ecr get-login-password --region eu-north-1 | docker login --username AWS --password-stdin 654654200977.dkr.ecr.eu-north-1.amazonaws.com;

        echo \"Pulling Docker images...\";
        docker pull 654654200977.dkr.ecr.eu-north-1.amazonaws.com/petp_rds_repo_test:latest;


        echo \"Relogging in to ECR...\";
        aws ecr get-login-password --region eu-north-1 | docker login --username AWS --password-stdin 654654200977.dkr.ecr.eu-north-1.amazonaws.com;
        
        docker pull 654654200977.dkr.ecr.eu-north-1.amazonaws.com/petp_redis_repo_test:latest;

        docker images

        echo \"Installing Docker Compose...\";
        sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-\$(uname -s)-\$(uname -m)\" -o /usr/local/bin/docker-compose;
        sudo chmod +x /usr/local/bin/docker-compose;

        echo \"Checking Docker version...\";
        docker --version;  
        echo \"Checking Docker Compose version...\";
        docker-compose --version;

        echo \"Copying docker-compose.yaml from S3...\";
        aws s3 cp s3://petp-tf-bucket-test/docker-compose.yaml /home/ec2-user/docker-compose.yaml;

        ls -l /home/ec2-user/

    

        echo \"Starting Docker Compose...\";
        sudo /usr/local/bin/docker-compose -f /home/ec2-user/docker-compose.yaml up -d;
        echo \"Docker Compose started successfully!\"

        docker-compose logs

        docker ps 

        docker ps -a
        
        '
      "

  needs:
  - terraform_job
  - docker_build_and_push    
  artifacts:
    paths:
      - variables.env  # Include the variables.env file
      - private_key.pem  # Transmit the key as an artifact
      - $TF_WORKING_DIR/.terraform  # Persist the .terraform directory
      - $TF_WORKING_DIR/terraform.tfstate.d/test/terraform.tfstate  # Persist the state file
  
  only:
    - master  # Only run this job on the master branch


terraform_destroy:
  stage: terraform

  before_script:
  - echo "Installing Terraform..."
  - apt-get update && apt-get install -y wget unzip
  - wget https://releases.hashicorp.com/terraform/${TF_VERSION}/terraform_${TF_VERSION}_linux_amd64.zip
  - unzip terraform_${TF_VERSION}_linux_amd64.zip -d /usr/local/bin/
  - chmod +x /usr/local/bin/terraform
  - terraform --version  # Check Terraform version


  script:
    - echo "Downloading artifacts for destruction..."
    - ls -la  # List files to check if artifacts are present
    - echo "Initializing Terraform for destruction..."
    - cd $TF_WORKING_DIR  # Change to the Terraform directory

    - terraform init  # Re-initialize Terraform to install required plugins
    - echo "Selecting 'test' workspace..."
    - terraform workspace select test  # Select the "test" workspace
    - echo "Destroying Terraform resources..."
    - terraform destroy -auto-approve  # Automatically approve destruction
    - echo "Cleaning up resources..."
    - rm -rf $TF_WORKING_DIR/.terraform  # Remove .terraform directory
    - rm -f $TF_WORKING_DIR/terraform.tfstate.d/test/terraform.tfstate  # Remove state file
    - echo "Removing private key..."
    - rm -f private_key.pem  # Remove the private key
  needs:
    - job: terraform_job
    - job: docker_build_and_push
    - job: deploy_application
      artifacts: true
  when: manual  # This ensures you trigger it manually
  only:
    - master  # Run this job only on the master branchhhgit 

